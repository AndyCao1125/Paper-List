# A Paper List of [Yanjie Ze](https://yanjieze.com/)

Topics:
- Learning
  - [Visual Reinforcement Learning](topics/visual_reinforcement_learning.md)
  - [3D Vision](topics/3d_vision.md)
  - [Reinforcement Learning](topics/reinforcement_learning.md)
  - [Visual Recognition](topics/visual_recognition.md)
  - [Generative Model](topics/generative_model.md)
  - [Self-Supervised Learning](topics/self_supervised_learning.md)
- Robotics
  - [Robotic Manipulation](topics/robotic_manipulation.md)
  - [Robotic Locomotion](topics/robotic_locomotion.md)
- [Miscellaneous](topics/misc.md)


Some personal claims and understandings:
- Self-supervised learning serves as a comprehensive framework for learning representations from unlabeled data. Often considered the 'cake' of machine learning, it can be broadly classified into two paradigms: generative modeling and discriminative modeling. Despite the recent remarkable advancements in generative modeling, which primarily focus on the quality of generation, I continue to perceive the issue through the lens of representation learning.
- Reinforcement learning, robotic manipulation, and computer vision are all intricately connected within the realm of robot learning. While I categorize them as distinct topics, it's worth noting that some papers may be relevant to multiple subjects.
- It's important to recognize that these topics are not mutually exclusive. For instance, visual reinforcement learning combines elements of visual recognition and reinforcement learning. Additionally, self-supervised learning can be found pervading various areas of the machine learning field.

# Recent Random Papers
- arXiv 2023.4, **DINOv2**: Learning Robust Visual Features without Supervision, [arXiv](https://arxiv.org/abs/2304.07193) / [Github](https://github.com/facebookresearch/dinov2)
- arXiv 2022.10, **Interactive Language**: Talking to Robots in Real Time, [Website](https://interactive-language.github.io/) / [Github](https://github.com/google-research/language-table)
- arXiv 2023.4, Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields, [Website](https://jonbarron.info/zipnerf/)
- arXiv 2023.4, SEEM: Segment Everything Everywhere All at Once, [arXiv](https://arxiv.org/abs/2304.06718) / [code](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)
- arXiv 2023.4, Internet Explorer: Targeted Representation Learning on the Open Web, [page](https://internet-explorer-ssl.github.io/) / [code](https://github.com/internet-explorer-ssl/internet-explorer)
- arXiv 2023.03, Consistency Models, [code](https://github.com/openai/consistency_models) / [arXiv](https://arxiv.org/abs/2303.01469)
- arXiv 2023.02, SceneDreamer: Unbounded 3D Scene Generation from 2D Image Collections, [code](https://github.com/FrozenBurning/SceneDreamer) / [page](https://scene-dreamer.github.io/)
- arXiv 2023.04, [Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442)
- ICLR 2023 notable, [NTFields: Neural Time Fields for Physics-Informed Robot Motion Planning](https://openreview.net/forum?id=ApF0dmi1_9K)
- arXiv 2023, [For Pre-Trained Vision Models in Motor Control, Not All Policy Learning Methods are Created Equal](https://arxiv.org/abs/2304.04591)
- code, [Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions](https://github.com/ayaanzhaque/instruct-nerf2nerf)
- arXiv 2023, Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection, [code](https://github.com/IDEA-Research/GroundingDINO) / [arXiv](https://arxiv.org/abs/2303.05499)
- arXiv 2023, [Zero-1-to-3: Zero-shot One Image to 3D Object](https://arxiv.org/abs/2303.11328)
- ICLR 2023, [Towards Stable Test-Time Adaptation in Dynamic Wild World](https://arxiv.org/abs/2302.12400)
- CVPR 2023 highlight, [Neural Volumetric Memory for Visual Locomotion Control](https://rchalyang.github.io/NVM/)
- arXiv 2023, [Segment Anything](https://segment-anything.com/)
- ICRA 2023, [DribbleBot: Dynamic Legged Manipulation in the Wild](https://gmargo11.github.io/dribblebot/)
- arXiv 2023, [Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)
- ICML 2022, [Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents](https://wenlong.page/language-planner/)
- arXiv 2023, VC-1: [Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?](https://eai-vc.github.io/)
- ICLR 2022, DroQ: [Dropout Q-Functions for Doubly Efficient Reinforcement Learning](https://arxiv.org/abs/2110.02034)
- arXiv 2023, [RoboPianist: A Benchmark for High-Dimensional Robot Control](https://kzakka.com/robopianist/)
- ICLR 2021, DDIM: [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502)
- arXiv 2023, [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- arXiv 2023, [Your Diffusion Model is Secretly a Zero-Shot Classifier](https://diffusion-classifier.github.io/)
- CVPR 2023 highlight, [F2-NeRF: Fast Neural Radiance Field Training with Free Camera Trajectories](https://totoro97.github.io/projects/f2-nerf/)
- arXiv 2023, [Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware](https://tonyzhaozh.github.io/aloha/)
- ICCV 2021, [Where2Act: From Pixels to Actions for Articulated 3D Objects](https://cs.stanford.edu/~kaichun/where2act/)
- CVPR 2019 oral, [Semantic Image Synthesis with Spatially-Adaptive Normalization](https://github.com/NVlabs/SPADE)
- RSS 2021, [RMA: Rapid Motor Adaptation for Legged Robots](https://ashish-kmr.github.io/rma-legged-robots/)

# Contact
If you have any questions or suggestions, please feel free to contact me at lastyanjieze@gmail.com .
