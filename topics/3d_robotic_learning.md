# 3D Robot Learning

- [arXiv 2024.03](https://arxiv.org/abs/2403.03954), **3D Diffusion Policy**: Generalizable Visuomotor Policy Learning via Simple 3D Representations, [Website](https://3d-diffusion-policy.github.io/)
- [arXiv 2023.08](https://arxiv.org/abs/2308.16891), **GNFactor**: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields, [website](https://yanjieze.com/GNFactor/)

## 4D Representations
- arXiv 2024.12, Scaling 4D Representations, [arXiv](https://arxiv.org/abs/2412.15212)
- arXiv 2024.12, **Stereo4D**: Learning How Things Move in 3D from Internet Stereo Videos, [website](https://stereo4d.github.io/)
- arXiv 2024.12, **MegaSaM**: Accurate, Fast, and Robust Structure and Motion from Casual Dynamic Videos, [website](https://mega-sam.github.io/)
- arXiv 2024.10, **MonST3R**: A Simple Approach for Estimating Geometry in the Presence of Motion, [website](https://monst3r-project.github.io/)
- arXiv 2024.05, **MoSca**: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds, [website](https://www.cis.upenn.edu/~leijh/projects/mosca/)


## 3D Representations
- arXiv 2025.01, **GPT4Scene**: Understand 3D Scenes from Videos with Vision-Language Models, [arXiv](https://arxiv.org/abs/2501.01428)
- arXiv 2024.12, **Prompting Depth Anything** for 4K Resolution Accurate Metric Depth Estimation, [website](https://promptda.github.io/)
- arXiv 2024.11, SIGGRAPH Asia 2024 best paper, **Quark**: Real-time, High-resolution, and General Neural View Synthesis, [website](https://quark-3d.github.io/)
- arXiv 2024.11, **3D Convex Splatting**: Radiance Field Rendering with 3D Smooth Convexes, [website](https://convexsplatting.github.io/)
- arXiv 2024.06, **TutteNet**: Injective 3D Deformations by Composition of 2D Mesh Deformations, [arXiv](https://arxiv.org/abs/2406.12121)
- arXiv 2024.06, **CoFie**: Learning Compact Neural Surface Representations with Coordinate Fields, [arXiv](https://arxiv.org/abs/2406.03417)
- arXiv 2024.05, Evolutive Rendering Models, [website](https://fnzhan.com/Evolutive-Rendering-Models/

## Tactile Representations
- arXiv 2024.12, **NormalFlow**: Fast, Robust, and Accurate Contact-based Object 6DoF Pose Tracking with Vision-based Tactile Sensors, [website](https://joehjhuang.github.io/normalflow/)
- arXiv 2024.12, **Tactile DreamFusion**: Exploiting Tactile Sensing for 3D Generation, [arXiv](https://arxiv.org/abs/2412.06785)

## Language-Aligned 3D Representations
- arXiv 2024.10,  **VLM-Grounder**: A VLM Agent for Zero-Shot 3D Visual Grounding, [arXiv](https://arxiv.org/abs/2410.13860)
- arXiv 2024.05, **Grounded 3D-LLM** with Referent Tokens, [website](https://groundedscenellm.github.io/grounded_3d-llm.github.io/)
- arXiv 2023.08, **3D-VisTA**: Pre-trained Transformer for 3D Vision and Text Alignment, [arXiv](https://arxiv.org/abs/2308.04352)
- arXiv 2023.08, **PointLLM**: Empowering Large Language Models to Understand Point Clouds, [website](https://runsenxu.com/projects/PointLLM/)
- arXiv 2023.05, **ULIP-2**: Towards Scalable Multimodal Pre-training for 3D Understanding, [github](https://github.com/salesforce/ULIP)
- arXiv 2022.12, **ULIP**: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding, [website](https://tycho-xue.github.io/ULIP/)
- arXiv 2022.11, **OpenScene**: 3D Scene Understanding with Open Vocabularies, [website](https://pengsongyou.github.io/openscene)
